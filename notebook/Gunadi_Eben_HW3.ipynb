{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Gunadi_Eben_HW3</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Eben Gunadi\n",
    "<br>\n",
    "Github Username: egunadi\n",
    "<br>\n",
    "USC ID: 3976793880"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification Part 1: Feature Creation/Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bootstrapped.bootstrap as bs\n",
    "import bootstrapped.stats_functions as bs_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AReM Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bending1_dataset1_filepath = '../data/AReM/bending1/dataset1.csv'\n",
    "bending1_dataset2_filepath = '../data/AReM/bending1/dataset2.csv'\n",
    "bending1_dataset3_filepath = '../data/AReM/bending1/dataset3.csv'\n",
    "bending1_dataset4_filepath = '../data/AReM/bending1/dataset4.csv'\n",
    "bending1_dataset5_filepath = '../data/AReM/bending1/dataset5.csv'\n",
    "bending1_dataset6_filepath = '../data/AReM/bending1/dataset6.csv'\n",
    "bending1_dataset7_filepath = '../data/AReM/bending1/dataset7.csv'\n",
    "\n",
    "bending2_dataset1_filepath = '../data/AReM/bending2/dataset1.csv'\n",
    "bending2_dataset2_filepath = '../data/AReM/bending2/dataset2.csv'\n",
    "bending2_dataset3_filepath = '../data/AReM/bending2/dataset3.csv'\n",
    "bending2_dataset4_filepath = '../data/AReM/bending2/dataset4.csv'\n",
    "bending2_dataset5_filepath = '../data/AReM/bending2/dataset5.csv'\n",
    "bending2_dataset6_filepath = '../data/AReM/bending2/dataset6.csv'\n",
    "\n",
    "cycling_dataset1_filepath = '../data/AReM/cycling/dataset1.csv'\n",
    "cycling_dataset2_filepath = '../data/AReM/cycling/dataset2.csv'\n",
    "cycling_dataset3_filepath = '../data/AReM/cycling/dataset3.csv'\n",
    "cycling_dataset4_filepath = '../data/AReM/cycling/dataset4.csv'\n",
    "cycling_dataset5_filepath = '../data/AReM/cycling/dataset5.csv'\n",
    "cycling_dataset6_filepath = '../data/AReM/cycling/dataset6.csv'\n",
    "cycling_dataset7_filepath = '../data/AReM/cycling/dataset7.csv'\n",
    "cycling_dataset8_filepath = '../data/AReM/cycling/dataset8.csv'\n",
    "cycling_dataset9_filepath = '../data/AReM/cycling/dataset9.csv'\n",
    "cycling_dataset10_filepath = '../data/AReM/cycling/dataset10.csv'\n",
    "cycling_dataset11_filepath = '../data/AReM/cycling/dataset11.csv'\n",
    "cycling_dataset12_filepath = '../data/AReM/cycling/dataset12.csv'\n",
    "cycling_dataset13_filepath = '../data/AReM/cycling/dataset13.csv'\n",
    "cycling_dataset14_filepath = '../data/AReM/cycling/dataset14.csv'\n",
    "cycling_dataset15_filepath = '../data/AReM/cycling/dataset15.csv'\n",
    "\n",
    "lying_dataset1_filepath = '../data/AReM/lying/dataset1.csv'\n",
    "lying_dataset2_filepath = '../data/AReM/lying/dataset2.csv'\n",
    "lying_dataset3_filepath = '../data/AReM/lying/dataset3.csv'\n",
    "lying_dataset4_filepath = '../data/AReM/lying/dataset4.csv'\n",
    "lying_dataset5_filepath = '../data/AReM/lying/dataset5.csv'\n",
    "lying_dataset6_filepath = '../data/AReM/lying/dataset6.csv'\n",
    "lying_dataset7_filepath = '../data/AReM/lying/dataset7.csv'\n",
    "lying_dataset8_filepath = '../data/AReM/lying/dataset8.csv'\n",
    "lying_dataset9_filepath = '../data/AReM/lying/dataset9.csv'\n",
    "lying_dataset10_filepath = '../data/AReM/lying/dataset10.csv'\n",
    "lying_dataset11_filepath = '../data/AReM/lying/dataset11.csv'\n",
    "lying_dataset12_filepath = '../data/AReM/lying/dataset12.csv'\n",
    "lying_dataset13_filepath = '../data/AReM/lying/dataset13.csv'\n",
    "lying_dataset14_filepath = '../data/AReM/lying/dataset14.csv'\n",
    "lying_dataset15_filepath = '../data/AReM/lying/dataset15.csv'\n",
    "\n",
    "sitting_dataset1_filepath = '../data/AReM/sitting/dataset1.csv'\n",
    "sitting_dataset2_filepath = '../data/AReM/sitting/dataset2.csv'\n",
    "sitting_dataset3_filepath = '../data/AReM/sitting/dataset3.csv'\n",
    "sitting_dataset4_filepath = '../data/AReM/sitting/dataset4.csv'\n",
    "sitting_dataset5_filepath = '../data/AReM/sitting/dataset5.csv'\n",
    "sitting_dataset6_filepath = '../data/AReM/sitting/dataset6.csv'\n",
    "sitting_dataset7_filepath = '../data/AReM/sitting/dataset7.csv'\n",
    "sitting_dataset8_filepath = '../data/AReM/sitting/dataset8.csv'\n",
    "sitting_dataset9_filepath = '../data/AReM/sitting/dataset9.csv'\n",
    "sitting_dataset10_filepath = '../data/AReM/sitting/dataset10.csv'\n",
    "sitting_dataset11_filepath = '../data/AReM/sitting/dataset11.csv'\n",
    "sitting_dataset12_filepath = '../data/AReM/sitting/dataset12.csv'\n",
    "sitting_dataset13_filepath = '../data/AReM/sitting/dataset13.csv'\n",
    "sitting_dataset14_filepath = '../data/AReM/sitting/dataset14.csv'\n",
    "sitting_dataset15_filepath = '../data/AReM/sitting/dataset15.csv'\n",
    "\n",
    "standing_dataset1_filepath = '../data/AReM/standing/dataset1.csv'\n",
    "standing_dataset2_filepath = '../data/AReM/standing/dataset2.csv'\n",
    "standing_dataset3_filepath = '../data/AReM/standing/dataset3.csv'\n",
    "standing_dataset4_filepath = '../data/AReM/standing/dataset4.csv'\n",
    "standing_dataset5_filepath = '../data/AReM/standing/dataset5.csv'\n",
    "standing_dataset6_filepath = '../data/AReM/standing/dataset6.csv'\n",
    "standing_dataset7_filepath = '../data/AReM/standing/dataset7.csv'\n",
    "standing_dataset8_filepath = '../data/AReM/standing/dataset8.csv'\n",
    "standing_dataset9_filepath = '../data/AReM/standing/dataset9.csv'\n",
    "standing_dataset10_filepath = '../data/AReM/standing/dataset10.csv'\n",
    "standing_dataset11_filepath = '../data/AReM/standing/dataset11.csv'\n",
    "standing_dataset12_filepath = '../data/AReM/standing/dataset12.csv'\n",
    "standing_dataset13_filepath = '../data/AReM/standing/dataset13.csv'\n",
    "standing_dataset14_filepath = '../data/AReM/standing/dataset14.csv'\n",
    "standing_dataset15_filepath = '../data/AReM/standing/dataset15.csv'\n",
    "\n",
    "walking_dataset1_filepath = '../data/AReM/walking/dataset1.csv'\n",
    "walking_dataset2_filepath = '../data/AReM/walking/dataset2.csv'\n",
    "walking_dataset3_filepath = '../data/AReM/walking/dataset3.csv'\n",
    "walking_dataset4_filepath = '../data/AReM/walking/dataset4.csv'\n",
    "walking_dataset5_filepath = '../data/AReM/walking/dataset5.csv'\n",
    "walking_dataset6_filepath = '../data/AReM/walking/dataset6.csv'\n",
    "walking_dataset7_filepath = '../data/AReM/walking/dataset7.csv'\n",
    "walking_dataset8_filepath = '../data/AReM/walking/dataset8.csv'\n",
    "walking_dataset9_filepath = '../data/AReM/walking/dataset9.csv'\n",
    "walking_dataset10_filepath = '../data/AReM/walking/dataset10.csv'\n",
    "walking_dataset11_filepath = '../data/AReM/walking/dataset11.csv'\n",
    "walking_dataset12_filepath = '../data/AReM/walking/dataset12.csv'\n",
    "walking_dataset13_filepath = '../data/AReM/walking/dataset13.csv'\n",
    "walking_dataset14_filepath = '../data/AReM/walking/dataset14.csv'\n",
    "walking_dataset15_filepath = '../data/AReM/walking/dataset15.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bending1_dataset1_df = pd.read_csv(bending1_dataset1_filepath, delimiter=',', encoding='utf-8')\n",
    "bending1_dataset2_df = pd.read_csv(bending1_dataset2_filepath, delimiter=',', encoding='utf-8')\n",
    "bending1_dataset3_df = pd.read_csv(bending1_dataset3_filepath, delimiter=',', encoding='utf-8')\n",
    "bending1_dataset4_df = pd.read_csv(bending1_dataset4_filepath, delimiter=',', encoding='utf-8')\n",
    "bending1_dataset5_df = pd.read_csv(bending1_dataset5_filepath, delimiter=',', encoding='utf-8')\n",
    "bending1_dataset6_df = pd.read_csv(bending1_dataset6_filepath, delimiter=',', encoding='utf-8')\n",
    "bending1_dataset7_df = pd.read_csv(bending1_dataset7_filepath, delimiter=',', encoding='utf-8')\n",
    "\n",
    "bending2_dataset1_df = pd.read_csv(bending2_dataset1_filepath, delimiter=',', encoding='utf-8')\n",
    "bending2_dataset2_df = pd.read_csv(bending2_dataset2_filepath, delimiter=',', encoding='utf-8')\n",
    "bending2_dataset3_df = pd.read_csv(bending2_dataset3_filepath, delimiter=',', encoding='utf-8')\n",
    "bending2_dataset4_df = pd.read_csv(bending2_dataset4_filepath, delimiter=',', encoding='utf-8')\n",
    "bending2_dataset5_df = pd.read_csv(bending2_dataset5_filepath, delimiter=',', encoding='utf-8')\n",
    "bending2_dataset6_df = pd.read_csv(bending2_dataset6_filepath, delimiter=',', encoding='utf-8')\n",
    "\n",
    "cycling_dataset1_df = pd.read_csv(cycling_dataset1_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset2_df = pd.read_csv(cycling_dataset2_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset3_df = pd.read_csv(cycling_dataset3_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset4_df = pd.read_csv(cycling_dataset4_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset5_df = pd.read_csv(cycling_dataset5_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset6_df = pd.read_csv(cycling_dataset6_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset7_df = pd.read_csv(cycling_dataset7_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset8_df = pd.read_csv(cycling_dataset8_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset9_df = pd.read_csv(cycling_dataset9_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset10_df = pd.read_csv(cycling_dataset10_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset11_df = pd.read_csv(cycling_dataset11_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset12_df = pd.read_csv(cycling_dataset12_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset13_df = pd.read_csv(cycling_dataset13_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset14_df = pd.read_csv(cycling_dataset14_filepath, delimiter=',', encoding='utf-8')\n",
    "cycling_dataset15_df = pd.read_csv(cycling_dataset15_filepath, delimiter=',', encoding='utf-8')\n",
    "\n",
    "lying_dataset1_df = pd.read_csv(lying_dataset1_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset2_df = pd.read_csv(lying_dataset2_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset3_df = pd.read_csv(lying_dataset3_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset4_df = pd.read_csv(lying_dataset4_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset5_df = pd.read_csv(lying_dataset5_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset6_df = pd.read_csv(lying_dataset6_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset7_df = pd.read_csv(lying_dataset7_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset8_df = pd.read_csv(lying_dataset8_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset9_df = pd.read_csv(lying_dataset9_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset10_df = pd.read_csv(lying_dataset10_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset11_df = pd.read_csv(lying_dataset11_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset12_df = pd.read_csv(lying_dataset12_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset13_df = pd.read_csv(lying_dataset13_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset14_df = pd.read_csv(lying_dataset14_filepath, delimiter=',', encoding='utf-8')\n",
    "lying_dataset15_df = pd.read_csv(lying_dataset15_filepath, delimiter=',', encoding='utf-8')\n",
    "\n",
    "sitting_dataset1_df = pd.read_csv(sitting_dataset1_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset2_df = pd.read_csv(sitting_dataset2_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset3_df = pd.read_csv(sitting_dataset3_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset4_df = pd.read_csv(sitting_dataset4_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset5_df = pd.read_csv(sitting_dataset5_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset6_df = pd.read_csv(sitting_dataset6_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset7_df = pd.read_csv(sitting_dataset7_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset8_df = pd.read_csv(sitting_dataset8_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset9_df = pd.read_csv(sitting_dataset9_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset10_df = pd.read_csv(sitting_dataset10_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset11_df = pd.read_csv(sitting_dataset11_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset12_df = pd.read_csv(sitting_dataset12_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset13_df = pd.read_csv(sitting_dataset13_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset14_df = pd.read_csv(sitting_dataset14_filepath, delimiter=',', encoding='utf-8')\n",
    "sitting_dataset15_df = pd.read_csv(sitting_dataset15_filepath, delimiter=',', encoding='utf-8')\n",
    "\n",
    "standing_dataset1_df = pd.read_csv(standing_dataset1_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset2_df = pd.read_csv(standing_dataset2_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset3_df = pd.read_csv(standing_dataset3_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset4_df = pd.read_csv(standing_dataset4_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset5_df = pd.read_csv(standing_dataset5_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset6_df = pd.read_csv(standing_dataset6_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset7_df = pd.read_csv(standing_dataset7_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset8_df = pd.read_csv(standing_dataset8_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset9_df = pd.read_csv(standing_dataset9_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset10_df = pd.read_csv(standing_dataset10_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset11_df = pd.read_csv(standing_dataset11_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset12_df = pd.read_csv(standing_dataset12_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset13_df = pd.read_csv(standing_dataset13_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset14_df = pd.read_csv(standing_dataset14_filepath, delimiter=',', encoding='utf-8')\n",
    "standing_dataset15_df = pd.read_csv(standing_dataset15_filepath, delimiter=',', encoding='utf-8')\n",
    "\n",
    "walking_dataset1_df = pd.read_csv(walking_dataset1_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset2_df = pd.read_csv(walking_dataset2_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset3_df = pd.read_csv(walking_dataset3_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset4_df = pd.read_csv(walking_dataset4_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset5_df = pd.read_csv(walking_dataset5_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset6_df = pd.read_csv(walking_dataset6_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset7_df = pd.read_csv(walking_dataset7_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset8_df = pd.read_csv(walking_dataset8_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset9_df = pd.read_csv(walking_dataset9_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset10_df = pd.read_csv(walking_dataset10_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset11_df = pd.read_csv(walking_dataset11_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset12_df = pd.read_csv(walking_dataset12_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset13_df = pd.read_csv(walking_dataset13_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset14_df = pd.read_csv(walking_dataset14_filepath, delimiter=',', encoding='utf-8')\n",
    "walking_dataset15_df = pd.read_csv(walking_dataset15_filepath, delimiter=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1, 2, and 3 in other folders as test data and other datasets as train data.\n",
    "\n",
    "bending1_train_df = pd.concat([bending1_dataset3_df, bending1_dataset4_df, bending1_dataset5_df, \n",
    "                               bending1_dataset6_df, bending1_dataset7_df], ignore_index=True)\n",
    "bending1_test_df = pd.concat([bending1_dataset1_df, bending1_dataset2_df], ignore_index=True)\n",
    "\n",
    "bending2_train_df = pd.concat([bending2_dataset3_df, bending2_dataset4_df, bending2_dataset5_df, \n",
    "                               bending2_dataset6_df], ignore_index=True)\n",
    "bending2_test_df = pd.concat([bending2_dataset1_df, bending2_dataset2_df], ignore_index=True)\n",
    "\n",
    "cycling_train_df = pd.concat([cycling_dataset4_df, cycling_dataset5_df, cycling_dataset6_df, \n",
    "                              cycling_dataset7_df, cycling_dataset8_df, cycling_dataset9_df, \n",
    "                              cycling_dataset10_df, cycling_dataset11_df, cycling_dataset12_df, \n",
    "                              cycling_dataset13_df, cycling_dataset14_df, cycling_dataset15_df], ignore_index=True)\n",
    "cycling_test_df = pd.concat([cycling_dataset1_df, cycling_dataset2_df, cycling_dataset3_df], ignore_index=True)\n",
    "\n",
    "lying_train_df = pd.concat([lying_dataset4_df, lying_dataset5_df, lying_dataset6_df, \n",
    "                            lying_dataset7_df, lying_dataset8_df, lying_dataset9_df, \n",
    "                            lying_dataset10_df, lying_dataset11_df, lying_dataset12_df, \n",
    "                            lying_dataset13_df, lying_dataset14_df, lying_dataset15_df], ignore_index=True)\n",
    "lying_test_df = pd.concat([lying_dataset1_df, lying_dataset2_df, lying_dataset3_df], ignore_index=True)\n",
    "\n",
    "sitting_train_df = pd.concat([sitting_dataset4_df, sitting_dataset5_df, sitting_dataset6_df, \n",
    "                              sitting_dataset7_df, sitting_dataset8_df, sitting_dataset9_df, \n",
    "                              sitting_dataset10_df, sitting_dataset11_df, sitting_dataset12_df, \n",
    "                              sitting_dataset13_df, sitting_dataset14_df, sitting_dataset15_df], ignore_index=True)\n",
    "sitting_test_df = pd.concat([sitting_dataset1_df, sitting_dataset2_df, sitting_dataset3_df], ignore_index=True)\n",
    "\n",
    "standing_train_df = pd.concat([standing_dataset4_df, standing_dataset5_df, standing_dataset6_df, \n",
    "                               standing_dataset7_df, standing_dataset8_df, standing_dataset9_df, \n",
    "                               standing_dataset10_df, standing_dataset11_df, standing_dataset12_df, \n",
    "                               standing_dataset13_df, standing_dataset14_df, standing_dataset15_df], ignore_index=True)\n",
    "standing_test_df = pd.concat([standing_dataset1_df, standing_dataset2_df, standing_dataset3_df], ignore_index=True)\n",
    "\n",
    "walking_train_df = pd.concat([walking_dataset4_df, walking_dataset5_df, walking_dataset6_df, \n",
    "                              walking_dataset7_df, walking_dataset8_df, walking_dataset9_df, \n",
    "                              walking_dataset10_df, walking_dataset11_df, walking_dataset12_df, \n",
    "                              walking_dataset13_df, walking_dataset14_df, walking_dataset15_df], ignore_index=True)\n",
    "walking_test_df = pd.concat([walking_dataset1_df, walking_dataset2_df, walking_dataset3_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research what types of time-domain features are usually used in time series classification and list them (examples are minimum, maximum, mean, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Time series classification usually leverage time-domain features such as:\n",
    "> \n",
    "> - Mean\n",
    "> - Standard Deviation\n",
    "> - Median\n",
    "> - Minimum/Maximum\n",
    "> - First/Third Quartiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_list = [bending1_dataset1_df, bending1_dataset2_df, bending1_dataset3_df, \n",
    "                bending1_dataset4_df, bending1_dataset5_df, bending1_dataset6_df, bending1_dataset7_df,\n",
    "                \n",
    "                bending2_dataset1_df, bending2_dataset2_df, bending2_dataset3_df, bending2_dataset4_df, bending2_dataset5_df, bending2_dataset6_df,\n",
    "                \n",
    "                cycling_dataset1_df, cycling_dataset2_df, cycling_dataset3_df, cycling_dataset4_df, cycling_dataset5_df, cycling_dataset6_df, cycling_dataset7_df, cycling_dataset8_df, cycling_dataset9_df, cycling_dataset10_df, cycling_dataset11_df, cycling_dataset12_df, cycling_dataset13_df, cycling_dataset14_df, cycling_dataset15_df,\n",
    "                \n",
    "                lying_dataset1_df, lying_dataset2_df, lying_dataset3_df, lying_dataset4_df, lying_dataset5_df, lying_dataset6_df, lying_dataset7_df, lying_dataset8_df, lying_dataset9_df, lying_dataset10_df, lying_dataset11_df, lying_dataset12_df, lying_dataset13_df, lying_dataset14_df, lying_dataset15_df,\n",
    "                \n",
    "                sitting_dataset1_df, sitting_dataset2_df, sitting_dataset3_df, sitting_dataset4_df, sitting_dataset5_df, sitting_dataset6_df, sitting_dataset7_df, sitting_dataset8_df, sitting_dataset9_df, sitting_dataset10_df, sitting_dataset11_df, sitting_dataset12_df, sitting_dataset13_df, sitting_dataset14_df, sitting_dataset15_df,\n",
    "                \n",
    "                standing_dataset1_df, standing_dataset2_df, standing_dataset3_df, standing_dataset4_df, standing_dataset5_df, standing_dataset6_df, standing_dataset7_df, standing_dataset8_df, standing_dataset9_df, standing_dataset10_df, standing_dataset11_df, standing_dataset12_df, standing_dataset13_df, standing_dataset14_df, standing_dataset15_df,\n",
    "                \n",
    "                walking_dataset1_df, walking_dataset2_df, walking_dataset3_df, walking_dataset4_df, walking_dataset5_df, walking_dataset6_df, walking_dataset7_df, walking_dataset8_df, walking_dataset9_df, walking_dataset10_df, walking_dataset11_df, walking_dataset12_df, walking_dataset13_df, walking_dataset14_df, walking_dataset15_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/r_7yspw94w77hrsrcwzwy75m0000gn/T/ipykernel_7694/3537725272.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  features_df = pd.concat([features_df, pd.DataFrame([instance_dict])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "features_df = pd.DataFrame(columns=['instance', \n",
    "                                    'min1', 'max1', 'mean1', 'median1', 'std1', '1st quartile1', '3rd quartile1', \n",
    "                                    'min2', 'max2', 'mean2', 'median2', 'std2', '1st quartile2', '3rd quartile2', \n",
    "                                    'min3', 'max3', 'mean3', 'median3', 'std3', '1st quartile3', '3rd quartile3', \n",
    "                                    'min4', 'max4', 'mean4', 'median4', 'std4', '1st quartile4', '3rd quartile4', \n",
    "                                    'min5', 'max5', 'mean5', 'median5', 'std5', '1st quartile5', '3rd quartile5', \n",
    "                                    'min6', 'max6', 'mean6', 'median6', 'std6', '1st quartile6', '3rd quartile6'])\n",
    "\n",
    "instance_counter = 1\n",
    "for df in full_df_list:\n",
    "    describe_df = df.describe()\n",
    "\n",
    "    instance_dict = {  'instance': instance_counter,\n",
    "                                  \n",
    "                        'min1': describe_df['avg_rss12']['min'], 'max1': describe_df['avg_rss12']['max'], 'mean1': describe_df['avg_rss12']['mean'], 'median1': describe_df['avg_rss12']['50%'], 'std1': describe_df['avg_rss12']['std'], '1st quartile1': describe_df['avg_rss12']['25%'], '3rd quartile1': describe_df['avg_rss12']['75%'],\n",
    "\n",
    "                        'min2': describe_df['var_rss12']['min'], 'max2': describe_df['var_rss12']['max'], 'mean2': describe_df['var_rss12']['mean'], 'median2': describe_df['var_rss12']['50%'], 'std2': describe_df['var_rss12']['std'], '1st quartile2': describe_df['var_rss12']['25%'], '3rd quartile2': describe_df['var_rss12']['75%'],\n",
    "\n",
    "                        'min3': describe_df['avg_rss13']['min'], 'max3': describe_df['avg_rss13']['max'], 'mean3': describe_df['avg_rss13']['mean'], 'median3': describe_df['avg_rss13']['50%'], 'std3': describe_df['avg_rss13']['std'], '1st quartile3': describe_df['avg_rss13']['25%'], '3rd quartile3': describe_df['avg_rss13']['75%'],\n",
    "\n",
    "                        'min4': describe_df['var_rss13']['min'], 'max4': describe_df['var_rss13']['max'], 'mean4': describe_df['var_rss13']['mean'], 'median4': describe_df['var_rss13']['50%'], 'std4': describe_df['var_rss13']['std'], '1st quartile4': describe_df['var_rss13']['25%'], '3rd quartile4': describe_df['var_rss13']['75%'],\n",
    "\n",
    "                        'min5': describe_df['avg_rss23']['min'], 'max5': describe_df['avg_rss23']['max'], 'mean5': describe_df['avg_rss23']['mean'], 'median5': describe_df['avg_rss23']['50%'], 'std5': describe_df['avg_rss23']['std'], '1st quartile5': describe_df['avg_rss23']['25%'], '3rd quartile5': describe_df['avg_rss23']['75%'],\n",
    "\n",
    "                        'min6': describe_df['var_rss23']['min'], 'max6': describe_df['var_rss23']['max'], 'mean6': describe_df['var_rss23']['mean'], 'median6': describe_df['var_rss23']['50%'], 'std6': describe_df['var_rss23']['std'], '1st quartile6': describe_df['var_rss23']['25%'], '3rd quartile6': describe_df['var_rss23']['75%']    }\n",
    "\n",
    "    features_df = pd.concat([features_df, pd.DataFrame([instance_dict])], ignore_index=True)\n",
    "\n",
    "    instance_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 43)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance</th>\n",
       "      <th>min1</th>\n",
       "      <th>max1</th>\n",
       "      <th>mean1</th>\n",
       "      <th>median1</th>\n",
       "      <th>std1</th>\n",
       "      <th>1st quartile1</th>\n",
       "      <th>3rd quartile1</th>\n",
       "      <th>min2</th>\n",
       "      <th>max2</th>\n",
       "      <th>...</th>\n",
       "      <th>std5</th>\n",
       "      <th>1st quartile5</th>\n",
       "      <th>3rd quartile5</th>\n",
       "      <th>min6</th>\n",
       "      <th>max6</th>\n",
       "      <th>mean6</th>\n",
       "      <th>median6</th>\n",
       "      <th>std6</th>\n",
       "      <th>1st quartile6</th>\n",
       "      <th>3rd quartile6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>37.25</td>\n",
       "      <td>45.00</td>\n",
       "      <td>40.624792</td>\n",
       "      <td>40.50</td>\n",
       "      <td>1.476967</td>\n",
       "      <td>39.25</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>...</td>\n",
       "      <td>2.188449</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.570583</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38.00</td>\n",
       "      <td>45.67</td>\n",
       "      <td>42.812812</td>\n",
       "      <td>42.50</td>\n",
       "      <td>1.435550</td>\n",
       "      <td>42.00</td>\n",
       "      <td>43.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995255</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>34.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.571083</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.601010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>35.00</td>\n",
       "      <td>47.40</td>\n",
       "      <td>43.954500</td>\n",
       "      <td>44.33</td>\n",
       "      <td>1.558835</td>\n",
       "      <td>43.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>...</td>\n",
       "      <td>1.999604</td>\n",
       "      <td>35.3625</td>\n",
       "      <td>36.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.493292</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.513506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33.00</td>\n",
       "      <td>47.75</td>\n",
       "      <td>42.179812</td>\n",
       "      <td>43.50</td>\n",
       "      <td>3.670666</td>\n",
       "      <td>39.15</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.849448</td>\n",
       "      <td>30.4575</td>\n",
       "      <td>36.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.613521</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.524317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33.00</td>\n",
       "      <td>45.75</td>\n",
       "      <td>41.678063</td>\n",
       "      <td>41.75</td>\n",
       "      <td>2.243490</td>\n",
       "      <td>41.33</td>\n",
       "      <td>42.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>...</td>\n",
       "      <td>2.411026</td>\n",
       "      <td>28.4575</td>\n",
       "      <td>31.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.383292</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.389164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  instance   min1   max1      mean1  median1      std1  1st quartile1  \\\n",
       "0        1  37.25  45.00  40.624792    40.50  1.476967          39.25   \n",
       "1        2  38.00  45.67  42.812812    42.50  1.435550          42.00   \n",
       "2        3  35.00  47.40  43.954500    44.33  1.558835          43.00   \n",
       "3        4  33.00  47.75  42.179812    43.50  3.670666          39.15   \n",
       "4        5  33.00  45.75  41.678063    41.75  2.243490          41.33   \n",
       "\n",
       "   3rd quartile1  min2  max2  ...      std5  1st quartile5  3rd quartile5  \\\n",
       "0          42.00   0.0  1.30  ...  2.188449        33.0000          36.00   \n",
       "1          43.67   0.0  1.22  ...  1.995255        32.0000          34.50   \n",
       "2          45.00   0.0  1.70  ...  1.999604        35.3625          36.50   \n",
       "3          45.00   0.0  3.00  ...  3.849448        30.4575          36.33   \n",
       "4          42.75   0.0  2.83  ...  2.411026        28.4575          31.25   \n",
       "\n",
       "   min6  max6     mean6  median6      std6  1st quartile6  3rd quartile6  \n",
       "0   0.0  1.92  0.570583     0.43  0.582915            0.0           1.30  \n",
       "1   0.0  3.11  0.571083     0.43  0.601010            0.0           1.30  \n",
       "2   0.0  1.79  0.493292     0.43  0.513506            0.0           0.94  \n",
       "3   0.0  2.18  0.613521     0.50  0.524317            0.0           1.00  \n",
       "4   0.0  1.79  0.383292     0.43  0.389164            0.0           0.50  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iii. Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/r_7yspw94w77hrsrcwzwy75m0000gn/T/ipykernel_7694/4206994341.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  std_devs = pd.concat([std_devs, pd.DataFrame([std_dev_dict])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>std_dev</th>\n",
       "      <th>lower_ci</th>\n",
       "      <th>upper_ci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>min1</td>\n",
       "      <td>9.568541</td>\n",
       "      <td>8.357328</td>\n",
       "      <td>10.880013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max1</td>\n",
       "      <td>4.183493</td>\n",
       "      <td>3.291839</td>\n",
       "      <td>5.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean1</td>\n",
       "      <td>5.246019</td>\n",
       "      <td>4.691703</td>\n",
       "      <td>5.839980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>median1</td>\n",
       "      <td>5.355577</td>\n",
       "      <td>4.744457</td>\n",
       "      <td>6.022398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>std1</td>\n",
       "      <td>1.761073</td>\n",
       "      <td>1.586866</td>\n",
       "      <td>1.978012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1st quartile1</td>\n",
       "      <td>6.092527</td>\n",
       "      <td>5.568175</td>\n",
       "      <td>6.702888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3rd quartile1</td>\n",
       "      <td>5.002031</td>\n",
       "      <td>4.281569</td>\n",
       "      <td>5.792559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>min2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max2</td>\n",
       "      <td>5.030493</td>\n",
       "      <td>4.675973</td>\n",
       "      <td>5.439284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mean2</td>\n",
       "      <td>1.568813</td>\n",
       "      <td>1.432639</td>\n",
       "      <td>1.748241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>median2</td>\n",
       "      <td>1.405398</td>\n",
       "      <td>1.281549</td>\n",
       "      <td>1.578147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>std2</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.825998</td>\n",
       "      <td>0.957125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1st quartile2</td>\n",
       "      <td>0.942967</td>\n",
       "      <td>0.854163</td>\n",
       "      <td>1.053976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3rd quartile2</td>\n",
       "      <td>2.119053</td>\n",
       "      <td>1.947779</td>\n",
       "      <td>2.348643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min3</td>\n",
       "      <td>2.937487</td>\n",
       "      <td>2.788638</td>\n",
       "      <td>3.122516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>max3</td>\n",
       "      <td>4.792067</td>\n",
       "      <td>4.192050</td>\n",
       "      <td>5.536233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean3</td>\n",
       "      <td>3.953833</td>\n",
       "      <td>3.428922</td>\n",
       "      <td>4.566573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>median3</td>\n",
       "      <td>3.986577</td>\n",
       "      <td>3.463057</td>\n",
       "      <td>4.571303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>std3</td>\n",
       "      <td>0.946524</td>\n",
       "      <td>0.758509</td>\n",
       "      <td>1.124345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1st quartile3</td>\n",
       "      <td>4.160032</td>\n",
       "      <td>3.641606</td>\n",
       "      <td>4.755767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3rd quartile3</td>\n",
       "      <td>4.129511</td>\n",
       "      <td>3.617842</td>\n",
       "      <td>4.766586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>min4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>max4</td>\n",
       "      <td>2.169234</td>\n",
       "      <td>1.995133</td>\n",
       "      <td>2.378210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mean4</td>\n",
       "      <td>1.161534</td>\n",
       "      <td>1.107866</td>\n",
       "      <td>1.254389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>median4</td>\n",
       "      <td>1.141971</td>\n",
       "      <td>1.088489</td>\n",
       "      <td>1.232312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>std4</td>\n",
       "      <td>0.455969</td>\n",
       "      <td>0.428601</td>\n",
       "      <td>0.494866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1st quartile4</td>\n",
       "      <td>0.840050</td>\n",
       "      <td>0.797030</td>\n",
       "      <td>0.909276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3rd quartile4</td>\n",
       "      <td>1.546271</td>\n",
       "      <td>1.473211</td>\n",
       "      <td>1.674842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>min5</td>\n",
       "      <td>6.085924</td>\n",
       "      <td>4.693364</td>\n",
       "      <td>7.736302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max5</td>\n",
       "      <td>5.740079</td>\n",
       "      <td>4.891335</td>\n",
       "      <td>6.710276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mean5</td>\n",
       "      <td>5.671149</td>\n",
       "      <td>4.597923</td>\n",
       "      <td>7.020662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>median5</td>\n",
       "      <td>5.810683</td>\n",
       "      <td>4.750432</td>\n",
       "      <td>7.171795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>std5</td>\n",
       "      <td>1.006523</td>\n",
       "      <td>0.788242</td>\n",
       "      <td>1.224395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1st quartile5</td>\n",
       "      <td>6.086856</td>\n",
       "      <td>5.001668</td>\n",
       "      <td>7.378900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3rd quartile5</td>\n",
       "      <td>5.531486</td>\n",
       "      <td>4.449894</td>\n",
       "      <td>6.668852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>min6</td>\n",
       "      <td>0.045835</td>\n",
       "      <td>0.013210</td>\n",
       "      <td>0.091670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>max6</td>\n",
       "      <td>2.518912</td>\n",
       "      <td>2.282149</td>\n",
       "      <td>2.775488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>mean6</td>\n",
       "      <td>1.150552</td>\n",
       "      <td>1.088193</td>\n",
       "      <td>1.236655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>median6</td>\n",
       "      <td>1.083576</td>\n",
       "      <td>1.027011</td>\n",
       "      <td>1.181497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>std6</td>\n",
       "      <td>0.513989</td>\n",
       "      <td>0.486902</td>\n",
       "      <td>0.554316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1st quartile6</td>\n",
       "      <td>0.757152</td>\n",
       "      <td>0.706320</td>\n",
       "      <td>0.825449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3rd quartile6</td>\n",
       "      <td>1.518071</td>\n",
       "      <td>1.441613</td>\n",
       "      <td>1.635085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature   std_dev  lower_ci   upper_ci\n",
       "0            min1  9.568541  8.357328  10.880013\n",
       "1            max1  4.183493  3.291839   5.246500\n",
       "2           mean1  5.246019  4.691703   5.839980\n",
       "3         median1  5.355577  4.744457   6.022398\n",
       "4            std1  1.761073  1.586866   1.978012\n",
       "5   1st quartile1  6.092527  5.568175   6.702888\n",
       "6   3rd quartile1  5.002031  4.281569   5.792559\n",
       "7            min2  0.000000  0.000000   0.000000\n",
       "8            max2  5.030493  4.675973   5.439284\n",
       "9           mean2  1.568813  1.432639   1.748241\n",
       "10        median2  1.405398  1.281549   1.578147\n",
       "11           std2  0.880769  0.825998   0.957125\n",
       "12  1st quartile2  0.942967  0.854163   1.053976\n",
       "13  3rd quartile2  2.119053  1.947779   2.348643\n",
       "14           min3  2.937487  2.788638   3.122516\n",
       "15           max3  4.792067  4.192050   5.536233\n",
       "16          mean3  3.953833  3.428922   4.566573\n",
       "17        median3  3.986577  3.463057   4.571303\n",
       "18           std3  0.946524  0.758509   1.124345\n",
       "19  1st quartile3  4.160032  3.641606   4.755767\n",
       "20  3rd quartile3  4.129511  3.617842   4.766586\n",
       "21           min4  0.000000  0.000000   0.000000\n",
       "22           max4  2.169234  1.995133   2.378210\n",
       "23          mean4  1.161534  1.107866   1.254389\n",
       "24        median4  1.141971  1.088489   1.232312\n",
       "25           std4  0.455969  0.428601   0.494866\n",
       "26  1st quartile4  0.840050  0.797030   0.909276\n",
       "27  3rd quartile4  1.546271  1.473211   1.674842\n",
       "28           min5  6.085924  4.693364   7.736302\n",
       "29           max5  5.740079  4.891335   6.710276\n",
       "30          mean5  5.671149  4.597923   7.020662\n",
       "31        median5  5.810683  4.750432   7.171795\n",
       "32           std5  1.006523  0.788242   1.224395\n",
       "33  1st quartile5  6.086856  5.001668   7.378900\n",
       "34  3rd quartile5  5.531486  4.449894   6.668852\n",
       "35           min6  0.045835  0.013210   0.091670\n",
       "36           max6  2.518912  2.282149   2.775488\n",
       "37          mean6  1.150552  1.088193   1.236655\n",
       "38        median6  1.083576  1.027011   1.181497\n",
       "39           std6  0.513989  0.486902   0.554316\n",
       "40  1st quartile6  0.757152  0.706320   0.825449\n",
       "41  3rd quartile6  1.518071  1.441613   1.635085"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate the standard deviation of each of the time-domain features you extracted from the data. Then, use Python’s bootstrapped or any other method to build a 90% bootsrap confidence interval for the standard deviation of each feature.\n",
    "\n",
    "std_devs = pd.DataFrame(columns=['feature', 'std_dev', 'lower_ci', 'upper_ci'])\n",
    "for column in features_df.columns:\n",
    "    if column != 'instance':\n",
    "        # Identify and handle missing values\n",
    "        data_cleaned = features_df[column].dropna()\n",
    "\n",
    "        # Convert the cleaned data to a Numpy array\n",
    "        data_array = data_cleaned.values\n",
    "\n",
    "        # Bootstrap method to estimate confidence intervals for each feature's standard deviation\n",
    "        bootstrapped_std_dev = bs.bootstrap(data_array, stat_func=bs_stats.std, alpha=0.1, num_iterations=1000)\n",
    "\n",
    "        # Append the results to the DataFrame\n",
    "        std_dev_dict = {'feature': column, 'std_dev': bootstrapped_std_dev.value, 'lower_ci': bootstrapped_std_dev.lower_bound, 'upper_ci': bootstrapped_std_dev.upper_bound}\n",
    "\n",
    "        std_devs = pd.concat([std_devs, pd.DataFrame([std_dev_dict])], ignore_index=True)\n",
    "\n",
    "std_devs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iv. Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your judgement to select the three most important time-domain features (one option may be min, mean, and max)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For me, the three most important time-domain features are:\n",
    "> \n",
    "> - Mean\n",
    "> - Standard Deviation\n",
    "> - Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ISLR 3.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β0 + β1X + β2X2 + β3X3 + ϵ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the true relationship between X and Y is linear, i.e. Y = β0 + β1X + ϵ. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the training RSS, I would expect cubic regression to be lower. Even when the true relationship is linear there is still irreducible error, which the cubic regression can provide a better fit for (more variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Linear Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer (a) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the test RSS, I would expect linear regression to be lower since it did not overfit the data during training (more bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Not Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the training RSS, I would still expect cubic regression to be lower. Regardless of the true relationship, the cubic regression can follow the points more closely and provide a better fit (more variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Not Linear Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer (c) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the test RSS, there is not enough information to tell which is lower since we don't know how far the true relationship is from linear: if closer to linear I would expect the lower one to be linear regression, but otherwise it'd be cubic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.3 - Extra Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a data set with five predictors, X1 = GPA, X2 = IQ, X3 = Level (1 for College and 0 for High School), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get ˆβ0 = 50, ˆβ1 = 20, ˆβ2 = 0.07, ˆβ3 = 35, ˆβ4 = 0.01, ˆβ5 = −10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Which answer is correct, and why?\n",
    "\n",
    "- i. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\n",
    "- ii. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\n",
    "- iii. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\n",
    "- iv. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Salary = 50 + 20(GPA) + 0.07(IQ) + 35(Level) + 0.01(GPA * IQ) - 10(GPA * Level)`\n",
    "\n",
    "Since IQ and GPA is fixed in this comparison, we can remove `20(GPA)`,  `0.07(IQ)`, and `0.01(GPA * IQ)`:\n",
    "\n",
    "`Salary = 50 + 35(Level) - 10(GPA * Level)`\n",
    "\n",
    "=> High School (Level 0): `Salary = 50` \n",
    "\n",
    "=> College (Level 1): `Salary = 50 + 35 - 10(GPA) = 85 - 10(GPA)`\n",
    "\n",
    "Assuming a 4.0 GPA, the college graduate would have a Salary of `85 - 10(4) = 85 - 40 = 45`, less than the high school graduate.\n",
    "\n",
    "Hence the correct answer is (iii)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Salary = 50 + 20(GPA) + 0.07(IQ) + 35(Level) + 0.01(GPA * IQ) - 10(GPA * Level)`\n",
    "\n",
    "=> College (Level 1): `Salary = 50 + 20(4) + 0.07(110) + 35 + 0.01(4 * 110) - 10(4) = 137.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False, statistical significance of a coefficient is not determined by its magnitude but by its p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.5 - Extra Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the fitted values that result from performing linear regression without an intercept. In this setting, the ith fitted value takes the form\n",
    "\n",
    "`ˆy_i = x_i * ˆβ`,\n",
    "\n",
    "where\n",
    "\n",
    "`ˆβ = ( n∑i=1 x_i * y_i ) / ( n∑i′=1 x_i′^2 )`. \n",
    "\n",
    "Show that we can write\n",
    "\n",
    "`ˆy_i = n∑i′=1 a_i′ * y_i′`.\n",
    "\n",
    "What is a_i′ ?\n",
    "\n",
    "> Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity, let's rewrite `ˆβ` with different index variables:\n",
    "\n",
    "`ˆβ = ( n∑i′=1 x_i′ * y_i′ ) / ( n∑k=1 x_k^2 )`. \n",
    "\n",
    "Substituting the expression for `ˆβ` into the equation for `ˆy_i`, we get:\n",
    "\n",
    "`ˆy_i = x_i * ( n∑i′=1 x_i′ * y_i′ ) / ( n∑k=1 x_k^2 )` \n",
    "\n",
    "Distributing `x_i` into the summation gives:\n",
    "\n",
    "`ˆy_i =  ( n∑i′=1 x_i′ * x_i * y_i′ ) / ( n∑k=1 x_k^2 )` \n",
    "\n",
    "And distributing `1 / ( n∑k=1 x_k^2 )` into the summation gives:\n",
    "\n",
    "=> `ˆy_i =  n∑i′=1 [(x_i′ * x_i) / ( n∑k=1 x_k^2 )] * y_i′` \n",
    "\n",
    "This turns into \n",
    "\n",
    "`ˆy_i = n∑i′=1 a_i′ * y_i′`\n",
    "\n",
    "given that\n",
    "\n",
    "`a_i′ = (x_i′ * x_i) / ( n∑k=1 x_k^2 )`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Citations\n",
    "\n",
    "- https://anaconda.org/silvertrix/bootstrapped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
